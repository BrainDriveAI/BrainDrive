{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "afb849cf-c1a1-408e-8f23-3dcece825982",
    "_uuid": "02afebe4-43e1-447c-a1ae-68d07c4d3066",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Fine-Tune Qwen3-0.6B and Qwen3-1.7B on Kaggle\n",
    "\n",
    "This notebook fine-tunes Qwen3-0.6B and Qwen3-1.7B models on a BrainDrive Q&A dataset using Hugging Face `transformers` and `peft` with LoRA. It runs on Kaggle's T4 x2 GPUs, installs dependencies, fetches models from Hugging Face, and converts the fine-tuned models to GGUF format for download.\n",
    "\n",
    "## Prerequisites\n",
    "- **Dataset**: Upload `braindrive_qa_dataset.jsonl` to Kaggle as a dataset (e.g., `/kaggle/input/braindrive-qa/braindrive_qa_dataset.jsonl`).\n",
    "- **Kaggle Settings**: Enable GPU (T4 x2) in the notebook settings.\n",
    "- **Output**: Fine-tuned models and GGUF files will be saved to `/kaggle/working/` for download.\n",
    "\n",
    "## Steps\n",
    "1. Install dependencies.\n",
    "2. Fine-tune Qwen3-0.6B and Qwen3-1.7B with LoRA.\n",
    "3. Convert models to GGUF format.\n",
    "4. Save outputs for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b9773ac5-2176-485f-aec2-0c7df476352a",
    "_uuid": "96bef46f-b3b7-4ed8-8e1d-641e1dd2b08c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-19T01:03:28.705330Z",
     "iopub.status.busy": "2025-05-19T01:03:28.705069Z",
     "iopub.status.idle": "2025-05-19T01:05:04.919000Z",
     "shell.execute_reply": "2025-05-19T01:05:04.918280Z",
     "shell.execute_reply.started": "2025-05-19T01:03:28.705303Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Collecting trl\n",
      "  Downloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (14.0.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (3.20.3)\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (1.34.1)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core) (1.70.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core) (2.40.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (4.9.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.17.0-py3-none-any.whl (348 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m249.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, trl, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.45.5 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.17.0\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (14.0.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (3.20.3)\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (1.34.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core) (1.70.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core) (2.40.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (4.9.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core) (2025.4.26)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core) (0.6.1)\n",
      "CUDA available: True\n",
      "GPU count: 2\n",
      "GPU name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install torch transformers peft trl datasets accelerate bitsandbytes numpy rich fsspec protobuf google-api-core huggingface_hub\n",
    "!pip install numpy rich fsspec protobuf google-api-core\n",
    "\n",
    "# Install llama.cpp without dependencies to avoid protobuf conflicts\n",
    "# !pip install git+https://github.com/ggerganov/llama.cpp.git --no-deps --no-cache-dir -q\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "983394ff-c4fd-4d92-a7a7-616edd4102a3",
    "_uuid": "82a40389-7364-4594-85b3-a24acfa54235",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Setup Logging and Imports\n",
    "\n",
    "Configure logging to track progress and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "317f47d7-1a75-4598-8868-970778a01674",
    "_uuid": "7703231f-8efc-4959-9be0-3a2665b052de",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-19T01:08:17.021534Z",
     "iopub.status.busy": "2025-05-19T01:08:17.021100Z",
     "iopub.status.idle": "2025-05-19T01:08:56.842986Z",
     "shell.execute_reply": "2025-05-19T01:08:56.842260Z",
     "shell.execute_reply.started": "2025-05-19T01:08:17.021486Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 01:08:32.917232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747616913.352057      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747616913.476987      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# File handler\n",
    "os.makedirs('/kaggle/working/logs', exist_ok=True)\n",
    "file_handler = RotatingFileHandler('/kaggle/working/logs/finetune_qwen3.log', maxBytes=10*1024*1024, backupCount=5)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f50105e-a447-4426-8372-f823fd8958bd",
    "_uuid": "9240630f-e63b-4e3e-ba9e-5e58344cd6cc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Define Utility Functions\n",
    "\n",
    "Define a function to check dataset token lengths and ensure compatibility with the model's max sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "cf184db0-b374-409a-b179-14b052099d58",
    "_uuid": "bc199d0e-d34c-4775-bdae-f7df9d257f0f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-19T01:09:38.844577Z",
     "iopub.status.busy": "2025-05-19T01:09:38.843503Z",
     "iopub.status.idle": "2025-05-19T01:09:38.849537Z",
     "shell.execute_reply": "2025-05-19T01:09:38.848844Z",
     "shell.execute_reply.started": "2025-05-19T01:09:38.844551Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_dataset_lengths(dataset_path, model_name, max_seq_length):\n",
    "    \"\"\"Check token lengths in dataset to validate max_seq_length.\"\"\"\n",
    "    logger.info(\"Checking dataset token lengths\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path)[\"train\"]\n",
    "    lengths = []\n",
    "    for example in dataset:\n",
    "        text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)\n",
    "        length = len(tokenizer(text).input_ids)\n",
    "        lengths.append(length)\n",
    "    max_length = max(lengths)\n",
    "    percentile_95 = sorted(lengths)[int(0.95 * len(lengths))]\n",
    "    logger.info(f\"Dataset token lengths: Max={max_length}, 95th percentile={percentile_95}\")\n",
    "    if max_length > max_seq_length:\n",
    "        logger.warning(f\"Some examples exceed max_seq_length={max_seq_length}. Consider increasing or truncating.\")\n",
    "    return max_length, percentile_95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8241b320-40f3-4ee8-838a-4b34a591cb38",
    "_uuid": "765616ce-2850-45a4-8536-2c00da706011",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Fine-Tune Function\n",
    "\n",
    "Define a reusable function to fine-tune a model with LoRA, save it, and convert to GGUF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "d0578494-9a4a-4f95-969e-862af4e2545f",
    "_uuid": "b7c81d3a-a232-41a2-a541-07c243dc1824",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-19T01:09:42.489472Z",
     "iopub.status.busy": "2025-05-19T01:09:42.489184Z",
     "iopub.status.idle": "2025-05-19T01:09:42.499938Z",
     "shell.execute_reply": "2025-05-19T01:09:42.499109Z",
     "shell.execute_reply.started": "2025-05-19T01:09:42.489454Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def finetune_model(model_name, output_dir, dataset_path, max_seq_length=2048):\n",
    "    \"\"\"Fine-tune a model with LoRA and save it.\"\"\"\n",
    "    logger.info(f\"Starting fine-tuning for {model_name}\")\n",
    "\n",
    "    # 4-bit quantization config\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch.float16,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    # )\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    logger.info(f\"Loading model and tokenizer: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        # quantization_config=bnb_config,\n",
    "        device_map={'': torch.cuda.current_device()},  # Explicitly place on primary GPU\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Configure LoRA\n",
    "    logger.info(\"Configuring LoRA adapters\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Load dataset\n",
    "    logger.info(\"Loading and preprocessing dataset\")\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n",
    "    # Check token lengths\n",
    "    max_length, percentile_95 = check_dataset_lengths(dataset_path, model_name, max_seq_length)\n",
    "\n",
    "    # Format dataset\n",
    "    def format_chat(example):\n",
    "        return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\n",
    "    dataset = dataset.map(format_chat, num_proc=4)\n",
    "\n",
    "    # Split dataset\n",
    "    logger.info(\"Splitting dataset for training and evaluation\")\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "    logger.info(f\"Dataset size: {len(train_dataset)} train, {len(eval_dataset)} eval\")\n",
    "\n",
    "    # Training arguments\n",
    "    logger.info(\"Setting up training arguments\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        fp16=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "\n",
    "    # Initialize data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Causal LM, not masked LM\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    logger.info(\"Initializing SFTTrainer\")\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    logger.info(\"Starting training\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model\n",
    "    logger.info(f\"Saving fine-tuned model to {output_dir}\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Note: GGUF conversion moved to local machine\n",
    "    logger.info(f\"Model saved to {output_dir}. GGUF conversion to be performed locally.\")\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0d63d4a2-1c6d-4ed3-a469-5da35986dbd0",
    "_uuid": "92de192a-0cbc-48bf-aa3b-bd820a0205e3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Fine-Tune Qwen3-0.6B\n",
    "\n",
    "Fine-tune the smaller Qwen3-0.6B model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "0eb8458c-2213-4a2a-b067-5cd8f1f73840",
    "_uuid": "4009b975-9c19-428f-86ab-f691655e797f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-19T01:11:06.412883Z",
     "iopub.status.busy": "2025-05-19T01:11:06.412586Z",
     "iopub.status.idle": "2025-05-19T01:36:40.378149Z",
     "shell.execute_reply": "2025-05-19T01:36:40.377363Z",
     "shell.execute_reply.started": "2025-05-19T01:11:06.412862Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 01:11:06,414 - INFO - Starting fine-tuning for Qwen/Qwen3-0.6B\n",
      "2025-05-19 01:11:06,416 - INFO - Loading model and tokenizer: Qwen/Qwen3-0.6B\n",
      "2025-05-19 01:11:07,947 - INFO - Configuring LoRA adapters\n",
      "2025-05-19 01:11:08,181 - INFO - Loading and preprocessing dataset\n",
      "2025-05-19 01:11:08,294 - INFO - Checking dataset token lengths\n",
      "2025-05-19 01:11:09,776 - INFO - Dataset token lengths: Max=280, 95th percentile=156\n",
      "2025-05-19 01:11:09,959 - INFO - Splitting dataset for training and evaluation\n",
      "2025-05-19 01:11:09,963 - INFO - Dataset size: 1089 train, 121 eval\n",
      "2025-05-19 01:11:09,964 - INFO - Setting up training arguments\n",
      "2025-05-19 01:11:09,993 - INFO - Initializing SFTTrainer\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "2025-05-19 01:11:11,020 - INFO - Starting training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 25:24, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.553200</td>\n",
       "      <td>2.426454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.882600</td>\n",
       "      <td>1.576433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.730200</td>\n",
       "      <td>1.490416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.801300</td>\n",
       "      <td>1.449658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>1.423204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.729600</td>\n",
       "      <td>1.404234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>1.365470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.764300</td>\n",
       "      <td>1.354662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.667100</td>\n",
       "      <td>1.350945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "2025-05-19 01:36:39,949 - INFO - Saving fine-tuned model to /kaggle/working/finetuned_qwen3-0.6B\n",
      "2025-05-19 01:36:40,329 - INFO - Model saved to /kaggle/working/finetuned_qwen3-0.6B. GGUF conversion to be performed locally.\n"
     ]
    }
   ],
   "source": [
    "# Dataset path (adjust to your Kaggle dataset path)\n",
    "dataset_path = \"/kaggle/input/braindrive-concierge-qa-pairs/braindrive_qa_dataset.jsonl\"\n",
    "\n",
    "# Fine-tune Qwen3-0.6B\n",
    "model_06b, tokenizer_06b = finetune_model(\n",
    "    model_name=\"Qwen/Qwen3-0.6B\",\n",
    "    output_dir=\"/kaggle/working/finetuned_qwen3-0.6B\",\n",
    "    dataset_path=dataset_path,\n",
    "    max_seq_length=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be7d37c3-a4bf-4bde-b706-9429916fb46a",
    "_uuid": "7f8ec71f-a74b-4709-a4e9-c18086083589",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Fine-Tune Qwen3-1.7B\n",
    "\n",
    "Fine-tune the larger Qwen3-1.7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c073de72-dd47-44a2-8e87-f22ca06a8817",
    "_uuid": "063c1707-ae43-4a33-b606-40a7aee53160",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-18T20:50:07.186391Z",
     "iopub.status.busy": "2025-05-18T20:50:07.185806Z",
     "iopub.status.idle": "2025-05-18T21:21:20.916155Z",
     "shell.execute_reply": "2025-05-18T21:21:20.915222Z",
     "shell.execute_reply.started": "2025-05-18T20:50:07.186361Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune Qwen3-1.7B\n",
    "model_17b, tokenizer_17b = finetune_model(\n",
    "    model_name=\"Qwen/Qwen3-1.7B\",\n",
    "    output_dir=\"/kaggle/working/finetuned_qwen3-1.7B\",\n",
    "    dataset_path=dataset_path,\n",
    "    max_seq_length=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6dbabb99-5481-4405-9ded-0ea7fa128aed",
    "_uuid": "98c9f3ce-9ae5-4748-aa4c-6c3e03f32ce1",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-05-19T01:10:52.547906Z",
     "iopub.status.idle": "2025-05-19T01:10:52.548248Z",
     "shell.execute_reply": "2025-05-19T01:10:52.548075Z",
     "shell.execute_reply.started": "2025-05-19T01:10:52.548062Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge Lora with base below!\n",
    "# !zip -r /kaggle/working/finetuned_qwen3_models.zip /kaggle/working/finetuned_qwen3-0.6B /kaggle/working/finetuned_qwen3-1.7B\n",
    "# print(\"Zipped models saved to /kaggle/working/finetuned_qwen3_models.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T01:47:55.162488Z",
     "iopub.status.busy": "2025-05-19T01:47:55.162012Z",
     "iopub.status.idle": "2025-05-19T01:48:00.504090Z",
     "shell.execute_reply": "2025-05-19T01:48:00.503285Z",
     "shell.execute_reply.started": "2025-05-19T01:47:55.162463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Loading LoRA adapters...\n",
      "Merging adapters...\n",
      "Saving merged model to /kaggle/working/merged_qwen3-0.6B...\n",
      "Merged model saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "base_model_name = \"Qwen/Qwen3-0.6B\" # \"Qwen/Qwen3-1.7B\"\n",
    "lora_adapter_path = \"/kaggle/working/finetuned_qwen3-0.6B\"\n",
    "merged_model_path = \"/kaggle/working/merged_qwen3-0.6B\"\n",
    "\n",
    "# Load base model and tokenizer\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load LoRA adapters\n",
    "print(\"Loading LoRA adapters...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "\n",
    "# Merge adapters with base model\n",
    "print(\"Merging adapters...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "print(f\"Saving merged model to {merged_model_path}...\")\n",
    "os.makedirs(merged_model_path, exist_ok=True)\n",
    "merged_model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "print(\"Merged model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T01:55:49.653909Z",
     "iopub.status.busy": "2025-05-19T01:55:49.653594Z",
     "iopub.status.idle": "2025-05-19T01:55:49.858526Z",
     "shell.execute_reply": "2025-05-19T01:55:49.857608Z",
     "shell.execute_reply.started": "2025-05-19T01:55:49.653882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/merged_qwen3_0.6B.zip\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/merged_qwen3_0.6B.zip /kaggle/working/merged_qwen3-0.6B/*\n",
    "!ls -dh /kaggle/working/merged_qwen3_0.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T01:57:47.589036Z",
     "iopub.status.busy": "2025-05-19T01:57:47.588741Z",
     "iopub.status.idle": "2025-05-19T01:57:47.594873Z",
     "shell.execute_reply": "2025-05-19T01:57:47.594328Z",
     "shell.execute_reply.started": "2025-05-19T01:57:47.589016Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this takes you to a 404 page > take a smoke break and come back...\n",
      "For whatever reason Kaggle takes it's sweet time getting this ready to download\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/kaggle/working/merged_qwen3_0.6B.zip' target='_blank'>/kaggle/working/merged_qwen3_0.6B.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/merged_qwen3_0.6B.zip"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "print(\"If this takes you to a 404 page > take a smoke break and come back...\\nFor whatever reason Kaggle takes it's sweet time getting this ready to download\")\n",
    "FileLink('/kaggle/working/merged_qwen3_0.6B.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T00:59:58.035777Z",
     "iopub.status.busy": "2025-05-19T00:59:58.035237Z",
     "iopub.status.idle": "2025-05-19T00:59:58.160772Z",
     "shell.execute_reply": "2025-05-19T00:59:58.159668Z",
     "shell.execute_reply.started": "2025-05-19T00:59:58.035742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Delete a single file\n",
    "# !rm /kaggle/working/finetuned_qwen3_models.zip\n",
    "\n",
    "# Delete the entire directory\n",
    "!rm -rf /kaggle/working/logs"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7454418,
     "sourceId": 11862763,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
